# src/data_pipeline/extractors/pms/pms_extractor.py
import asyncio
import random
from datetime import datetime, timezone, timedelta
import logging
from typing import Dict, Any, List, Optional, Callable
from abc import abstractmethod

from ..abstract_extractor import AbstractExtractor, ExtractionResult
from ....utils.env_utils import get_config
from ....utils.state_manager import load_last_run_timestamp, save_last_run_timestamp
from ....utils.date_params import DateWindow, ICT

from ...clients.pms_client import PMSClient 

logger = logging.getLogger(__name__)

class PMSExtractor(AbstractExtractor):
    """Base PMS extractor: qu·∫£n l√Ω client & khung extract chung."""

    PARSERS: Dict[str, Callable[[Any], Any]] = {}

    TOKEN_BRANCH_MAP = {
        1:  "KIN HOTEL DONG DU",
        2:  "KIN HOTEL THI SACH EDITION",
        3:  "KIN HOTEL THAI VAN LUNG",
        4:  "KIN WANDER TAN BINH, THE MOUNTAIN",
        5:  "KIN WANDER TAN QUY",
        6:  "KIN WANDER TAN PHONG, THE MOONAGE",
        7:  "KIN WANDER TRUNG SON",
        9:  "KIN HOTEL CENTRAL PARK",
        10: "KIN HOTEL LY TU TRONG",
    }

    def __init__(self):
        super().__init__("PMS")
        cfg = get_config().get('pms', {})
        base_url = (cfg.get('base_url') or '').rstrip('/') + '/'
        self.client = PMSClient(base_url)

    # --------- Facade sang client (cho backward-compat) ---------
    async def _get_session(self, branch_id: int):
        return await self.client.get_session(branch_id)

    async def _make_request(self, session, url: str, params: Dict[str, Any]):
        return await self.client.get_json(session, url, params)

    async def _paginate(self, session, url: str, base_params: Dict[str, Any], *, limit_default: int = 100):
        # url ƒë∆∞a v√†o ƒë√¢y l√† d·∫°ng ƒë·∫ßy ƒë·ªß; ƒë·ªÉ t√°i d√πng client, ta tr√≠ch endpoint
        # nh∆∞ng ƒë·ªÉ ƒë∆°n gi·∫£n & kh√¥ng ƒë·ª•ng nhi·ªÅu, cho ph√©p chuy·ªÅn endpoint tr·ª±c ti·∫øp t·ª´ l·ªõp con:
        # n·∫øu url ƒë√£ l√† base_url + endpoint, ta ch·ªâ c·∫ßn g·ªçi client.paginate_json v·ªõi endpoint
        # => t√°ch endpoint t·ª´ url:
        endpoint = url.split(self.client.base_url)[-1]
        return await self.client.paginate_json(session, endpoint, base_params, limit_default=limit_default)

    async def close(self):
        await self.client.close()

    # ---------------- Template extract ----------------
    @abstractmethod
    async def _perform_extraction(self, session, branch_id: int, **kwargs) -> List[Any]:
        pass

    async def extract_async(self, *, branch_id: int, **kwargs) -> ExtractionResult:
        start_time = datetime.now(timezone.utc)
        branch_name = self.TOKEN_BRANCH_MAP.get(branch_id, f"Branch {branch_id}")
        source_name = f"PMS:{getattr(self, 'ENDPOINT', 'Unknown')}"

        try:
            session = await self._get_session(branch_id)
            self.logger.info(f"üöÄ Starting extraction for {branch_name} - {source_name}")

            all_records = await self._perform_extraction(session, branch_id, **kwargs)

            duration = (datetime.now(timezone.utc) - start_time).total_seconds()
            self.logger.info(
                f"‚úÖ Extracted {len(all_records)} records from {branch_name} for endpoint '{source_name}' in {duration:.2f}s."
            )

            return ExtractionResult(
                data=all_records,
                source=source_name,
                branch_id=branch_id,
                branch_name=branch_name,
                record_count=len(all_records),
                **kwargs,
            )
        except Exception as e:
            duration = (datetime.now(timezone.utc) - start_time).total_seconds()
            self.logger.error(f"‚ùå Extraction failed for {branch_name} after {duration:.2f}s: {e}")
            return ExtractionResult(
                data=None,
                source=source_name,
                branch_id=branch_id,
                branch_name=branch_name,
                status="error",
                error=str(e),
                **kwargs,
            )

    def validate_config(self) -> bool:
        return bool(self.client and self.client.base_url)
    
        # ===== NEW: d·ª±ng DateWindow t·ª´ watermark state c√≥ s·∫µn =====
    def _build_window_from_state(
        self,
        *,
        endpoint: str,      # v√≠ d·ª•: "bookings"
        field: str,         # "check_in" ho·∫∑c "create"
        branch_id: int,
        lookback_days: int = 1
    ) -> DateWindow:
        """
        L·∫•y watermark t·ª´ state (UTC). N·∫øu ch∆∞a c√≥ ‚Üí d√πng now - lookback_days.
        C·ª≠a s·ªï: (last_run + 1s) ‚Üí now  ƒë·ªÉ tr√°nh tr√πng m√©p.
        """
        source_key = f"{endpoint}:{field}"
        now_utc = datetime.now(timezone.utc)

        last_run_utc = load_last_run_timestamp(source_key, branch_id)
        if last_run_utc:
            start_utc = last_run_utc + timedelta(seconds=1)
        else:
            start_utc = now_utc - timedelta(days=lookback_days)

        return DateWindow.from_utc(start=start_utc, end=now_utc, field=field, tz=ICT)

    # ===== NEW: incremental cho nhi·ªÅu branch, d√πng watermark chu·∫©n =====
    async def extract_incremental(
        self,
        *,
        endpoint: str,              # v√≠ d·ª• "bookings"
        field: str = "check_in",    # ho·∫∑c "create"
        branch_ids: Optional[List[int]] = None,
        lookback_days: int = 1,
        max_concurrent: int = 5,
        jitter_max_s: float = 0.8,
        **kwargs
    ) -> Dict[int, ExtractionResult]:
        """
        - T·∫°o DateWindow t·ª´ state (per endpoint+field+branch).
        - G·ªçi extract_async(branch_id, date_window=dw, **kwargs) cho t·ª´ng branch.
        - N·∫øu OK ‚Üí l∆∞u watermark = dw.end (UTC) b·∫±ng state_manager hi·ªán t·∫°i.
        """
        if branch_ids is None:
            branch_ids = list(self.TOKEN_BRANCH_MAP.keys())

        sem = asyncio.Semaphore(max_concurrent)
        results: Dict[int, ExtractionResult] = {}

        async def _run_one(bid: int):
            async with sem:
                try:
                    dw = self._build_window_from_state(
                        endpoint=endpoint, field=field, branch_id=bid, lookback_days=lookback_days
                    )
                    # Tr√°nh c√πng n·ªï v√†o page 1 ‚Üí gi·∫£m d√≠nh WAF
                    await asyncio.sleep(random.random() * jitter_max_s)

                    res = await self.extract_async(branch_id=bid, date_window=dw, **kwargs)
                    results[bid] = res

                    if res.is_success:
                        # L∆ØU √ù: state_manager hi·ªán t·∫°i nh·∫≠n ƒë·ªëi s·ªë theo ƒë√∫ng th·ª© t·ª± (kh√¥ng keyword)
                        save_last_run_timestamp(f"{endpoint}:{field}", bid, dw.end.astimezone(timezone.utc))
                except Exception as e:
                    results[bid] = ExtractionResult(
                        data=None,
                        source=f"PMS:{endpoint}",
                        branch_id=bid,
                        branch_name=self.TOKEN_BRANCH_MAP.get(bid, f"Branch {bid}"),
                        status="error",
                        error=str(e),
                    )

        await asyncio.gather(*(_run_one(b) for b in branch_ids))
        return results

    # ===== NEW: tri·ªÉn khai abstract extract_multi_branch ƒë·ªÉ h·∫øt TypeError =====
    async def extract_multi_branch(
        self,
        *,
        endpoint: str,
        branch_ids: Optional[List[int]] = None,
        field: str = "check_in",
        lookback_days: int = 1,
        max_concurrent: int = 5,
        **kwargs
    ) -> Dict[int, ExtractionResult]:
        """
        Wrapper m·ªèng g·ªçi extract_incremental() ƒë·ªÉ th·ªèa abstract method c·ªßa AbstractExtractor.
        """
        return await self.extract_incremental(
            endpoint=endpoint,
            field=field,
            branch_ids=branch_ids,
            lookback_days=lookback_days,
            max_concurrent=max_concurrent,
            **kwargs
        )

